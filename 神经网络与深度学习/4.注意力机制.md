# 注意力机制

## 软性注意力机制（Soft Attention）

**基本原理**：  
软性注意力机制通过可微的 softmax 操作计算所有输入位置的权重，生成上下文向量作为加权平均。这种机制允许模型动态关注输入的不同部分，适用于全局且可微分的场景

**步骤**：

1. **输入**：查询向量 $q$（如解码器当前状态）和一组键向量 $\set{k_ 1, k_ 2, \dots, k_ n}$ (如编码器隐藏状态)
2. **相似度计算**：计算查询 $q$ 与每个键 $k_ i$ 的相似度得分 $e_ i$. 常用方法包括点积 $e_ i = q \cdot k_ i$ 或加性模型（如神经网络）
3. **归一化权重**：对得分 $\set{e_ i}$ 应用 softmax，得到归一化的注意力权重 $\alpha_ i = \frac{\exp(e_ i)}{\sum_ j \exp(e_ j)}$
4. **加权求和**：将权重 $\alpha_ i$ 作用于对应的向量 $k_ i$, 生成上下文向量 $c = \sum_ i \alpha_ i k_ i$

### 打分函数

加性模型

$$
s(x,q)=v^T \tanh (Wx+Uq)
$$

点积模型

$$
s(x,q)=x^Tq
$$

缩放点积模型

$$
s(x,q)=\frac{x^Tq}{\sqrt{D}}
$$

双线性模型

$$
s(x,q)=x^TWq
$$

## 键值对注意力机制（Key-Value Attention）

**基本原理**：  
明确区分键（Key）和值（Value）的角色，键用于计算与查询的相关性，值用于生成最终的上下文向量。这种机制常用于编码器 - 解码器架构，键和值可来自不同变换。

**步骤**：

1. **输入**：查询 $q$，键集合 $\set{k_ 1, \dots, k_ n}$ 和对应的值集合 $\set{v_1, \dots, v_n}$
2. **相似度计算**：计算 $q$ 与每个键 $k_ i$ 的得分 $e_ i$，常用点积或缩放点积
3. **归一化权重**：对得分 $\set{e_ i}$ 应用 softmax，得到 $\alpha_ i$
4. **加权求和**：用权重 $\alpha_ i$ 对值 $v_ i$ 加权求和，得到上下文向量 $c = \sum_ i \alpha_ i v_ i$

## 多头注意力机制

利用多个查询 $Q=[q_1,\cdots,q_M]$, 来并行地从输入信息中选取多组信息。每个注意力关注输入信息的不同部分

$$
att((K,V),Q)=att((K,V),q_ 1) \oplus \cdots \oplus att((K,V),q_ M)
$$

## 自注意力机制（Self-Attention）

**基本原理**：  
序列中的每个元素通过自身生成查询（Query）、键（Key）、值（Value），计算序列内部元素的依赖关系，捕捉长距离关联。这是 Transformer 的核心模块。

**步骤**：

1. **线性变换**：输入序列 $\set{x_1, \dots, x_n}$ 通过三个矩阵 $W^Q, W^K, W^V$ 生成查询 $q_ i = W^Q x_ i$, 键 $k_ i = W^K x_ i$, 值 $v_ i = W^V x_ i$
2. **缩放点积**：对每个查询 $q_i$, 计算与所有键 $k_j$ 的点积并缩放: $e_ {ij} = \frac{q_ i \cdot k_ j}{\sqrt{d_k}}$, 其中 $d_k$ 为键向量维度
3. **归一化权重**：对每行 $\set{e_{i1}, \dots, e_{in}}$ 应用 softmax，得到权重 $\alpha_ {ij}$
4. **加权求和**：对每个位置 $i$，输出 $z_ i = \sum_ j \alpha_ {ij} v_ j$
5. **多头扩展（可选）**：并行执行多次自注意力，拼接结果后通过线性层融合

## 自注意力机制与 CNN、RNN 之间的关系

1. 自注意力机制可以看作是一种增强的 CNN，具有比 CNN 更大的感受野，以及动态生成的权值
2. 自注意力机制是一种可以捕捉长距离信息的 RNN，并且可以并行计算，因此可以提高计算效率
