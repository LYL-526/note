# 无监督学习

指从无标签的数据中学习出一些有用的模式

- 聚类: 建立映射关系 $f:x\rightarrow y$ (不借助于任何人工给出标签或者反馈等指导信息)
- 特征学习
- 密度估计 $p(x)$

## 主成分分析

一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大

$$
\begin{aligned}
z^{(n)}=w^Tx^{(n)} \\
\text{s.t.} \quad w^T w = 1
\end{aligned}
$$

$$
\begin{aligned}
\sigma(X;m) &=\frac{1}{N}\sum^N_ {n-1}(w^Tx^{(n)}-w^T\overline{x})^2 \\
 &=w^T X X^T w
\end{aligned}
$$

目标函数

$$
\max_ {w}w^T X X^T w+\lambda (1-w^Tw)
$$

$$
X X^T w=\lambda w
$$

### 算法

输入: $n$ 维样本集 $D = (x^{(1)}, x^{(2)}, ..., x^{(m)})$, 要降维到的维数 $n'$

输出: 降维后的样本集 $D'$

- 对所有的样本进行中心化: $x^{(i)} = x^{(i)} - \frac{1}{m}\sum_ {j=1}^{m}x^{(j)}$
- 计算样本的协方差矩阵 $XX^T$
- 对矩阵 $XX^T$ 进行特征值分解
- 取出最大的 $n'$ 个特征值对应的特征向量 $(w_ 1, w_ 2, ..., w_ {n'})$, 将所有的特征向量标准化后, 组成特征向量矩阵 $W$
- 对样本集中的每一个样本 $x^{(i)}$ , 转化为新的样本 $z^{(i)} = W^Tx^{(i)}$
- 得到输出样本集 $D' = (z^{(1)}, z^{(2)}, ..., z^{(m)})$

## 线性编码

给定一组基向量 $A=[a_ 1,\dots,a_ M]$, 将输入样本 $x$ 表示为这些基向量的线性组合

$$
x=Az
$$

### 稀疏编码

$$
\mathcal{L}(A,Z)=\sum_ {n=1}^N \left(\lVert x^{(n)}-Az^{(n)} \rVert^2 +\eta\rho(z^{(n)}) \right)
$$

## 自编码器

**原理**：自编码器（Autoencoder, AE）是一种无监督神经网络模型，用于学习数据的低维表示（编码）。其核心结构包括两部分：

- **编码器（Encoder）**：将输入数据映射到潜在空间（即编码），通常通过非线性变换实现降维：
  $$
  h = f(W_e x + b_e)
  $$
- **解码器（Decoder）**：从编码重建原始输入数据：
  $$
  \hat{x} = g(W_d h + b_d)
  $$

**目标函数**：最小化输入数据 $x$ 与重构数据 $\hat{x}$ 之间的差异，常用均方误差（MSE）或交叉熵损失：

$$
\mathcal{L} = \|x - \hat{x}\|^2
$$

**求解方法**：

- 使用梯度下降法（如 Adam、SGD）结合反向传播优化网络参数 $W_e, b_e, W_d, b_d$
- 通过迭代更新参数，使重构误差最小化

### 稀疏自编码器

**原理**：稀疏自编码器（Sparse Autoencoder, SAE）在标准自编码器基础上引入稀疏性约束，迫使隐藏层神经元在大部分时间内处于非活跃状态，从而学习更鲁棒的特征

**目标函数**：在重构损失基础上增加稀疏正则项：

$$
\mathcal{L} = \|x - \hat{x}\|^2 + \eta \rho(Z)+\lambda \|W\|^2
$$

$z$ 为隐藏层单元, $W$ 表示自编码器中的参数

#### 求解方法

- 同样使用梯度下降法，但需计算稀疏正则项的梯度
- 通过调整超参数 $\lambda$ 控制稀疏性强度

#### 优点

- 计算量：稀疏性带来的最大好处就是可以极大地降低计算量
- 可解释性：因为稀疏编码只有少数的非零元素，相当于将一个输入样本表示为少数几个相关的特征．这样我们可以更好地描述其特征，并易于理解
- 特征选择：稀疏性带来的另外一个好处是可以实现特征的自动选择，只选择和输入样本最相关的少数特征，从而更高效地表示输入样本，降低噪声并减轻过拟合

### 栈式（堆叠）自编码器

**原理**：栈式自编码器（Stacked Autoencoder, SAE）由多个自编码器逐层堆叠而成，用于学习多层次特征表示

**训练步骤**：

1. **逐层预训练**：分别训练每一层自编码器，将前一层编码器的输出作为下一层的输入
2. **整体微调**：堆叠所有编码器和解码器后，以有标签数据通过监督学习（如交叉熵损失）微调整个网络（若用于分类任务）

**特点**：

- 深层结构可捕获数据的高阶抽象特征
- 预训练缓解了深度网络梯度消失问题，提升模型泛化能力

### 降噪自编码器

**原理**：降噪自编码器（Denoising Autoencoder, DAE）通过向输入数据注入噪声，强制模型学习从损坏数据中恢复原始数据的能力，从而提高鲁棒性

**训练过程**：

1. **噪声注入**：对输入 $x$ 施加噪声生成 $\tilde{x}$（如随机掩蔽、高斯噪声）
2. **重构目标**：以带噪输入 $\tilde{x}$ 重建原始干净数据 $x$

**目标函数**：

$$
\mathcal{L} = \|x - \hat{x}\|^2
$$

其中

$$
\hat{x} = g(f(\tilde{x}))
$$

**特点**：

- 迫使编码器提取对噪声不敏感的特征，提升模型鲁棒性
- 常见噪声类型包括随机置零（Dropout）、高斯噪声等

## 自监督学习

旨在对于无标签数据，通过设计辅助任务（Proxy tasks）来挖掘数据自身的表征特性作为监督信息，来提升模型的特征提取能力

- 自然语言处理
  - 单词预测
  - 句子序列预测
  - 词序列预测
- 计算机视觉
  - 图像重组
  - 图像渲染
  - 图像旋转角度预测
  - 图像修复
  - 多任务学习
- 视频处理任务
  - 基于视频帧的序列信息
  - 基于视频中目标的相似性
  - 基于无监督目标跟踪

## 概率密度估计

### 参数密度估计

根据先验知识假设随机变量服从某种分布，然后通过训练样本来估计分布的参数

#### 问题

- 模型选择问题：即如何选择数据分布的密度函数．实际数据的分布往往是非常复杂的，而不是简单的正态分布或多项分布
- 不可观测变量问题：即我们用来训练的样本只包含部分的可观测变量，还有一些非常关键的变量是无法观测的，这导致我们很难准确估计数据的真实分布
- 维度灾难问题：即高维数据的参数估计十分困难．随着维度的增加，估计参数所需要的样本数量指数增加．在样本不足时会出现过拟合

### 非参数密度估计

不假设数据服从某种分布，通过将样本空间划分为不同的区域并估计每个区域的概率来近似数据的概率密度函数

## 独立模型训练方法

### 集成学习

对于 M 个不同的模型 $f_1,f_2,...,f_M$ ，通过集成学习将它们的预测结果结合起来，得到最终的预测结果

- 平均法：$f(x) = \frac{1}{M}\sum_ {m=1}^M f_m(x)$
- 投票法：$f(x) = \argmax_ {m=1}^M f_m(x)$

### 自训练

自训练（Self-Training，或 Self-Teaching），也叫自举法（Bootstrapping），是一种非常简单的半监督学习算法。自训练是首先使用标注数据来训练一个模型，并使用这个模型来预测无标注样本的标签，把预测置信度比较高的样本及其预测的伪标签加入训练集，然后重新训练新的模型，并不断重复这个过程

### 协同训练

协同训练（co-training）算法是一种多学习器的半监督学习算法。它是多视图（multi-view）学习的代表。以电影为例，它拥有多个属性集：图像、声音、字幕等。每个属性集就构成了一个视图。协同训练假设不同的视图具有“相容性”

所谓相容性是指，如果一个电影从图像判断，像是动作片，那么从声音判断，也有很大可能是动作片

协同训练的算法流程：

1. 在每个视图上，基于有标记数据，分别训练出一个分类器
2. 每个分类器挑选自己最有把握的未标记数据，赋予伪标记
3. 其他分类器利用伪标记，进一步训练模型，最终达到相互促进的目的

与自训练的主要区别在于多模型，多视图地训练

### 多任务学习

多任务学习 (Multi-task Learning) 是指同时学习多个相关任务，让这些任务在学习过程中共享知识，利用多个任务之间的相关性来改进模型在每个任务上的性能和泛化能力

### 迁移学习

迁移学习是指两个不同领域的知识迁移过程，利用源领域（SourceDomain）中学到的知识来帮助目标领域（Target Domain）上的学习任务，源领域的训练样本数量一般远大于目标领域

具体迁移学习往往分为两个步骤：

1. 根据超大规模数据，对模型进行训练
2. 根据具体场景任务进行微调（可以微调权重，还可以调整终端的结构）

### 终身学习

终身学习（Lifelong Learning），也叫持续学习（Continous Learning）是指像人类一样具有持续不断的学习能力，根据历史任务中学到的经验和知识来帮助学习不断出现的新任务，并且这些经验和知识是持续累积的，不会因为新的任务而忘记旧的知识

### 元学习

在元学习中，训练单位是任务，一般有两个任务分别是训练任务（Train Tasks）亦称跨任务（Across Tasks）和测试任务（Test Task）亦称单任务（Within Task）

训练任务要准备许多子任务来进行学习，目的是学习出一个较好的超参数，测试任务是利用训练任务学习出的超参数对特定任务进行训练。训练任务中的每个任务的数据分为 Support set 和 Query set；Test Task 中数据分为训练集和测试集
