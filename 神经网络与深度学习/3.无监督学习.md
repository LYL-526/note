# 无监督学习

指从无标签的数据中学习出一些有用的模式
- 聚类: 建立映射关系 $f:x\rightarrow y$ (不借助于任何人工给出标签或者反馈等指导信息)
- 特征学习
- 密度估计 $p(x)$

## 主成分分析

一种最常用的数据降维方法，使得在转换后的空间中数据的方差最大

$$
\begin{aligned}
z^{(n)}=w^Tx^{(n)} \\
\text{s.t.} \quad w^T w = 1  
\end{aligned}
$$

$$
\begin{aligned}
\sigma(X;m) &=\frac{1}{N}\sum^N_ {n-1}(w^Tx^{(n)}-w^T\overline{x})^2 \\
 &=w^T X X^T w
\end{aligned}
$$

目标函数

$$
\max_ {w}w^T X X^T w+\lambda (1-w^Tw)
$$

$$
X X^T w=\lambda w
$$

### 算法

输入: $n$ 维样本集 $D = (x^{(1)}, x^{(2)}, ..., x^{(m)})$, 要降维到的维数 $n'$

输出: 降维后的样本集 $D'$

- 对所有的样本进行中心化: $x^{(i)} = x^{(i)} - \frac{1}{m}\sum_ {j=1}^{m}x^{(j)}$
- 计算样本的协方差矩阵 $XX^T$
- 对矩阵 $XX^T$ 进行特征值分解
- 取出最大的 $n'$ 个特征值对应的特征向量 $(w_ 1, w_ 2, ..., w_ {n'})$, 将所有的特征向量标准化后, 组成特征向量矩阵 $W$
- 对样本集中的每一个样本 $x^{(i)}$ , 转化为新的样本 $z^{(i)} = W^Tx^{(i)}$
- 得到输出样本集 $D' = (z^{(1)}, z^{(2)}, ..., z^{(m)})$

## 线性编码

给定一组基向量 $A=[a_ 1,\dots,a_ M]$, 将输入样本 $x$ 表示为这些基向量的线性组合

$$
x=Az
$$

### 稀疏编码

$$
\mathcal{L}(A,Z)=\sum_ {n=1}^N \left(\lVert x^{(n)}-Az^{(n)} \rVert^2 +\eta\rho(z^{(n)}) \right)
$$

## 自编码器

**原理**：自编码器（Autoencoder, AE）是一种无监督神经网络模型，用于学习数据的低维表示（编码）。其核心结构包括两部分：  
- **编码器（Encoder）**：将输入数据映射到潜在空间（即编码），通常通过非线性变换实现降维：  
$$
h = f(W_e x + b_e)
$$  
- **解码器（Decoder）**：从编码重建原始输入数据：  
$$
\hat{x} = g(W_d h + b_d)
$$  

**目标函数**：最小化输入数据 $x$ 与重构数据 $\hat{x}$ 之间的差异，常用均方误差（MSE）或交叉熵损失：

$$
\mathcal{L} = \|x - \hat{x}\|^2
$$  

**求解方法**：  
- 使用梯度下降法（如Adam、SGD）结合反向传播优化网络参数 $W_e, b_e, W_d, b_d$
- 通过迭代更新参数，使重构误差最小化

### 稀疏自编码器

**原理**：稀疏自编码器（Sparse Autoencoder, SAE）在标准自编码器基础上引入稀疏性约束，迫使隐藏层神经元在大部分时间内处于非活跃状态，从而学习更鲁棒的特征

**目标函数**：在重构损失基础上增加稀疏正则项：  

$$
\mathcal{L} = \|x - \hat{x}\|^2 + \eta \rho(Z)+\lambda \|W\|^2
$$

$z$ 为隐藏层单元, $W$ 表示自编码器中的参数

**求解方法**：  
- 同样使用梯度下降法，但需计算稀疏正则项的梯度
- 通过调整超参数 $\lambda$ 控制稀疏性强度

### 栈式（堆叠）自编码器

**原理**：栈式自编码器（Stacked Autoencoder, SAE）由多个自编码器逐层堆叠而成，用于学习多层次特征表示

**训练步骤**：  
1. **逐层预训练**：分别训练每一层自编码器，将前一层编码器的输出作为下一层的输入
2. **整体微调**：堆叠所有编码器和解码器后，以有标签数据通过监督学习（如交叉熵损失）微调整个网络（若用于分类任务）

**特点**：  
- 深层结构可捕获数据的高阶抽象特征
- 预训练缓解了深度网络梯度消失问题，提升模型泛化能力

### 降噪自编码器

**原理**：降噪自编码器（Denoising Autoencoder, DAE）通过向输入数据注入噪声，强制模型学习从损坏数据中恢复原始数据的能力，从而提高鲁棒性

**训练过程**：  
1. **噪声注入**：对输入 $x$ 施加噪声生成 $\tilde{x}$（如随机掩蔽、高斯噪声）
2. **重构目标**：以带噪输入 $\tilde{x}$ 重建原始干净数据 $x$

**目标函数**：  

$$
\mathcal{L} = \|x - \hat{x}\|^2
$$

其中

$$
\hat{x} = g(f(\tilde{x}))
$$

**特点**：  
- 迫使编码器提取对噪声不敏感的特征，提升模型鲁棒性
- 常见噪声类型包括随机置零（Dropout）、高斯噪声等
