# 机器学习

## 机器学习三要素

1. 模型
2. 学习准则: 期望风险最小
3. 优化: 梯度下降

## 损失函数

### 0-1 损失函数

$$
\mathcal{L}(y,f(x,\theta))=
\begin{cases}
    0,& \text{if }y=f(x,\theta) \\
    1,& \text{if }y\neq f(x,\theta)
\end{cases}
$$

### 平方损失函数

$$
\mathcal{L}(y,\hat{y})=(y-f(x,\theta))^2
$$

## 经验风险

$$
R^{emp}_ D(\theta)=\frac{1}{N}\sum_ {n=1}^{N}L(y^{(n)},f(x^{(n)},\theta))
$$

## 随机梯度下降法

$$
\theta_{t+1}=\theta_t-\alpha \frac{\partial \mathcal{L}(\theta_t;x^{(t)}, y^{(t)})}{\partial\theta}
$$

> [!NOTE]
> 衍生算法: 小批量随机梯度下降法

## 泛化错误

$$
\text{泛化错误}=\text{期望风险}-\text{经验风险}
$$

$$
\mathcal{G}_ {\mathcal{D}}(f)=\mathcal{R}(f)-\mathcal{R}^{emp}_ {\mathcal{D}}(f)
$$

## 正则化

所有损害优化的方法都是正则化

- 增加优化约束
  - L1/L2 约束数据增强
- 干扰优化过程
  - 权重衰减
  - 提前停止
  - 随机梯度下降

## 优化方法

- 经验风险最小化
  - 最小二乘法
- 结构风险最小化
  - 岭回归
- 最大似然估计
- 最大后验估计

# 前馈神经网络

> [!NOTE]
> TODO

# 反向传播算法

## 优化问题

- 参数过多, 影响训练
- 非凸优化问题
- 梯度消失问题
- 参数解释起来比较困难

# 卷积神经网络

### 特性

- 局部连接
- 权重共享
- 空间或时间上的次采样

### 卷积结果按输出长度不同可以分为三类

设步长 $S$, 输入长度 $n$, 卷积核长度 $m$

- 窄卷积: $s=1$, 两端不补零 $p=0$, 卷积结果长度 $n-m+1$
- 宽卷积: $s=1$, 两端不补零 $p=m-1$, 卷积结果长度 $n+m-1$
- 等宽卷积: $s=1$, 两端不补零 $p=\frac{m-1}{2}$, 卷积结果长度 $n$

> [!NOTE]
> **卷积**运算需要翻转卷积核，**互相关**运算不需要翻转核；而卷积网络中卷积核为可学习参数，因此两者等价。卷积网络中的卷积一般指互相关

## 卷积层

卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器，在图像卷积中，为了更充分地利用图像地局部信息，通常将神经元组织为三维结构的神经层，其大小为 $\text{高度} M \times \text{宽度} N \times \text{深度} D$, 有 $D$ 个 $M \times N$大小地特征映射构成

在输入层，特征映射就是图像本身，如果是灰度图像，就是有一个特征映射，深度D=1；如果是彩色图像，分别有RGB三个颜色通道的特征映射，输入层深度D=3

假设一个卷积层结构如下：

- 输入特征映射组: $X\in R^{M\times N\times D}$ 为三维张量, 其中每个切片矩阵 $X^d\in R^{M\times N}$ 为一个输入特征映射, $1\leqslant d\leqslant D$
- 输出特征映射组: $Y\in R^{M'\times N'\times p}$ 为三维张量, 其中每个切片矩阵 $Y^d\in R^{M'\times N'}$ 为一个输出特征映射, $1\leqslant p \leqslant P$
- 卷积核: $W\in R^{m\times n\times D\times P}$ 为四维张量, 其中每个切片矩阵 $W^{p,d}\in R^{m\times n}$ 为一个两维卷积核, $1\leqslant D, 1\leqslant p \leqslant P$

最终表达式

$$
\begin{align}
  & Z^p=W^p \otimes X+b^p=\sum_{d=1}^{D} W^{p,d}\otimes X^d +b^p \\
  & Y^p=f(Z^p) \\
\end{align}
$$

$f(\cdot)$ 为激活函数

## 汇聚层

作用: 降低特征维数，避免过拟合

常用的汇聚函数
- 最大汇聚(Maximun Pooling): 取一个区域内所有神经元的最大值
- 平均汇聚(Mean Pooling): 一般是取区域内所有神经元的平均值

## 反向传播算法

> [!NOTE]
> TODO

# 循环神经网络

> [!NOTE]
> TODO

# 网络优化与正则化
