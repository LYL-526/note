# 机器学习

## 机器学习三要素

1. 模型
2. 学习准则: 期望风险最小
3. 优化: 梯度下降

## 损失函数

### 0-1 损失函数

$$
\mathcal{L}(y,f(x,\theta))=
\begin{cases}
    0,& \text{if }y=f(x,\theta) \\
    1,& \text{if }y\neq f(x,\theta)
\end{cases}
$$

### 平方损失函数

$$
\mathcal{L}(y,\hat{y})=(y-f(x,\theta))^2
$$

## 经验风险

$$
R^{emp}_D(\theta)=\frac{1}{N}\sum_{n=1}^{N}L(y^{(n)},f(x^{(n)},\theta))
$$

## 随机梯度下降法

$$
\theta_{t+1}=\theta_t-\alpha \frac{\partial \mathcal{L}(\theta_t;x^{(t)}, y^{(t)})}{\partial\theta}
$$

> [!NOTE]
> 衍生算法: 小批量随机梯度下降法

## 泛化错误

$$
\text{泛化错误}=\text{期望风险}-\text{经验风险}
$$

$$
\mathcal{G}_{\mathcal{D}}(f)=\mathcal{R}(f)-\mathcal{R}^{emp}_{\mathcal{D}}(f)
$$

## 正则化

所有损害优化的方法都是正则化

- 增加优化约束
  - L1/L2 约束数据增强
- 干扰优化过程
  - 权重衰减
  - 提前停止
  - 随机梯度下降

## 优化方法

- 经验风险最小化
  - 最小二乘法
- 结构风险最小化
  - 岭回归
- 最大似然估计
- 最大后验估计

# 前馈神经网络

> [!NOTE]
> TODO

# 反向传播算法

## 优化问题

- 参数过多, 影响训练
- 非凸优化问题
- 梯度消失问题
- 参数解释起来比较困难

# 卷积神经网络

> [!NOTE]
> TODO

# 循环神经网络

> [!NOTE]
> TODO

# 网络优化与正则化
