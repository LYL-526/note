# 概率图模型

## 含隐变量的参数学习

### 点估计

#### 最大似然

$$
\hat{\theta} _{MLE}=\argmax_ \theta p(x|\theta)
$$

##### EM 算法推导

$$
\begin{aligned}

p(X|\theta)&=\frac{p(X,\theta)}{p(\theta)}=\frac{\frac{p(X,z,\theta)}{p(\theta)}}{\frac{p(z,X,\theta)}{p(X,\theta)}}=\frac{p(X,z|\theta)}{p(z|X,\theta)} \\

\ln p(X|\theta)&=\ln \frac{p(X,z|\theta)}{q(z)}-\ln \frac{p(z|X,\theta)}{q(z)} \\

q(z)\ln p(X|\theta)&=q(z)\ln \frac{p(X,z|\theta)}{q(z)}-q(z)\ln \frac{p(z|X,\theta)}{q(z)} \\

\int q(z)\ln p(X|\theta) \mathrm{d}z&=\int q(z)\ln \frac{p(X,z|\theta)}{q(z)} \mathrm{d}z - \int q(z)\ln \frac{p(z|X,\theta)}{q(z)} \mathrm{d}z \\

\int q(z)\ln p(X|\theta) \mathrm{d}z&=\int q(z)\ln \frac{p(X,z|\theta)}{q(z)} \mathrm{d}z + \int q(z)\ln \frac{q(z)}{p(z|X,\theta)} \mathrm{d}z \\

\ln p(X|\theta)&=E_ {z \sim q(z)} \left[\ln \frac{p(X,z|\theta)}{q(z)} \right]  + KL \left[q(z)||p(z|X,\theta)\right]

\end{aligned}
$$

E 步

$$
\min_ {q(z)} KL \left[q(z)||p(z|X,\theta)\right]
$$

M 步

$$
\max_ \theta B = E_ {z \sim q(z)} \left[\ln \frac{p(X,z|\theta)}{q(z)} \right]=E_ {z \sim q(z)} \left[\ln p(x|z,\theta) \right] - KL \left[q(z)||p(z)\right]
$$

#### 最大后验估计

$$
\hat{\theta} _{MAP}=\argmax_ \theta p(x|\theta,\alpha)
$$

### KL 散度

对于两个概率分布 $P, Q$, 若为离散分布, 定义从 $P$ 到 $Q$ 的 KL 散度为

$$
D_ {KL}(P||Q)=\sum_ i P(i)\ln \frac{P(i)}{Q(i)}
$$

若为连续分布, 则

$$
D_ {KL}(P||Q)=\int^{\infty}_ {-\infty} P(x)\ln \frac{P(x)}{Q(x)} \mathrm{d}x
$$

#### 性质

1. 非负性: $D_ {KL}(P||Q) \geq 0$; $D_ {KL}(P||Q)=0$ 当且仅当 $P=Q$
2. 仿射变换不变性: 设 $y=ax+b$, 则 $D_ {KL}(P(x)||Q(x))=D_ {KL}(P(y)||Q(y))$
3. 非对易性: $D_ {KL}(P||Q)\neq D_ {KL}(Q||P)$

## 推断

推断是指在观测到部分变量 $e=\set{e_ 1, e_ 2, \cdots,e_ M}$ 时, 计算其他变量的某个子集 $q=\set{q_ 1, q_ 2, \cdots, q_ M}$ 的条件概率 $p(q|e)$

### 近似推断

#### 变分推断

变分法是17世纪末发展起来的一个数学分支, 主要研究变分问题, 即泛函的极值问题

假设在一个贝叶斯模型中, $x$ 为一组观测变量, $z$ 为一组隐变量, 我们的推断问题为计算条件概率密度 $p(z|x)$

$$
p(z|x)=\frac{p(x,z)}{p(x)}=\frac{p(x,z)}{\int p(x,z)\mathrm{d}z}
$$

对于很多模型来说, 计算上面公式中的积分是不可行的, 要么积分没有闭式解, 要么是指数级的计算复杂度

变分推断是变分法在推断问题中的应用, 是寻找一个简单分布 $q^*(x)$ 来近似条件概率密度 $p(z|x)$, 也称为 变分贝叶斯。这样, 推断问题转换为一个泛函优化问题：

$$
q^*(z)=\argmin_ {q(z)\in Q} KL(q(z)||p(z|x))
$$

由上 EM 算法可得

$$
\begin{aligned}
    q^*(z)
    &=\argmin_ {q(z)\in Q} \left(\ln p(x) - E_ {z \sim q(z)} \left[\ln p(x|z) \right]\right) \\
    &=\argmax_ {q(z)\in Q} E_ {z \sim q(z)} \left[\ln p(x|z) \right]
\end{aligned}
$$

#### 采样法（蒙特卡罗方法）

通过随机采样来近似估计一些计算问题数值解的方法。随机采样指从给定概率密度函数 $p(x)$ 中抽取出符合其概率分布的样本

采样法的难点是如何进行随机采样, 即如何让计算机生成满足概率密度函数 $p(x)$ 的样本

##### 拒绝采样

假设原始分布 $p(x)$ 难以直接采样, 我们可以引入一个容易采样的分布 $q(x)$, 一般称为提议分布, 然后以某个标准来拒绝一部分的样本使得最终采集的样本服从分布 $p(x)$

在拒绝采样中, 已知未归一化的分布 $\hat{p}(x)$, 我们需要构建一个提议分布 $q(x)$ 和一个常数 $k$, 使得 $kq(x)$ 可以覆盖函数 $\hat{p}(x)$, 即 $\forall x, kq(x)\geq \hat{p}(x)$

判断一个拒绝采样方法的好坏就是看其采样效率, 即总体的接受率．如果函数 $kq(x)$ 远大于原始分布函数 $\hat{p}(x)$, 拒绝率会比较高, 采样效率会非常不理想．但要找到一个和 $\hat{p}(x)$ 比较接近的提议分布往往比较困难．特别是在高维空间中, 其采样率会非常低, 导致很难应用到实际问题中

##### 重要性采样

如果采样的目的是计算分布 $p(x)$ 下函数 $f(x)$ 的期望，那么实际上抽取的样本不需要严格服从分布 $p(x)$, 也可以通过另一个分布，即提议分布 $q(x)$, 直接采样并估计 $E_ p[f(x)]$

$$
\begin{aligned}
    E_ p[f(x)]
    &=\int f(x)p(x)\mathrm{d}x \\
    &=\int f(x)\frac{p(x)}{q(x)}q(x)\mathrm{d}x \\
    &=\int f(x)\omega(x)q(x)\mathrm{d}x \\
    &=E_ q[f(x)\omega(x)]
\end{aligned}
$$

其中 $\omega(x)$ 称为重要性权重

重要性采样是通过引入重要性权重，将分布 $p(x)$ 下 $f(x)$ 的期望变为在分布 $q(x)$ 下 $f(x)\omega(x)$ 的期望，从而可以近似为

$$
\hat{f}_ N=\frac{1}{N}\left(f(x^{(1)})\omega(x^{(1)}) + \cdots + f(x^{(N)})\omega(x^{(N)}) \right)
$$

## 变分自编码器

在 EM 算法中，理论最优的 $q(z;\phi)$ 的概率密度为 $p(z|x,\theta)$

$$
p(z|x,\theta)=\frac{p(x|z,\theta)p(z|\theta)}{\int p(x|z,\theta)p(z|\theta)\mathrm{d}z}
$$

在一般情况下，这个后验概率密度函数是很难计算的

变分自编码器是利用神经网络来分别建模两个复杂的条件概率密度函数

1. 用神经网络来估计变分分布 $q(z;\phi)$, 称为推断网络。理论上 $q(z;\phi)$ 可以不依赖 $x$, 但由于 $q(z;\phi)$ 的目标是近似后验分布 $p(z|x,\theta)$, 其和 $x$ 相关，因此变分密度函数一般写为 $q(z|x;\phi)$, 推断网络的输入为 $x$, 输出为变分分布 $q(z|x;\phi)$

2. 用神经网络来估计概率分布 $p(x|z,\theta)$, 称为生成网络。生成网络的输入为 $z$，输出为概率分布 $p(x|z,\theta)$

变分自编码器的名称来自于其整个网络结构和自编码器比较类似
